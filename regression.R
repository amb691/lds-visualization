# Final Project: Regression
# Patrick Grennan
# Jason Schapiro

require(DBI)
require(RSQLite)
require(lars)
require(lasso2)

"
This project takes in the clusters from the first project, randomly selects 5
clusters, takes the mean of them, and then fits a model to the cluster using
the most efficient/parsimonious number of predictors from the list of TFs.

Functions:
1) main: This is the main function that orchestrates the above description.
This function makes calls to functions 2,3,4,5 and 10.
2) getClusters: after having the connection to the SQLite db in main(), this
function takes 5 random clusters from the database, finds the mean of the
each time point in each cluster, and then returns a list of the 5 vectors
representing the mean of each cluster.
3) tfList: This function loads the transcription factors from the ratios
data frame, and returns a matrix of just the TFs.
4) predictors: This takes in the matrix of TFs generated by tfList, and
retruns the 25 most correlated transcription factors to a given y, which
is the mean of a cluster i that is generated by getClusters.
5) bestFit(): Best fit is the function from homework 5: it returns the
model that has the fewest predictors and is within one standard error
of the best possible error.
*** Helper Functions ***
6) cv.folds(): cv.folds is a cross validation tool that splits the data
7) cv.lm(): cv.lm is a cross validated lm tool
8) plot.cv.lm(): plots the lm
9) predict.from.lm(): predicts from the lm
10)pause(): a simple pause function

"

main <- function(clusnum){
  # Connecting to the cluster database
  dbFile <- "clara.sqlite"
  conn <<- dbConnect(dbDriver("SQLite"), dbname = dbFile)
  sqlcom <- paste("select out from k173 where row_names = \"", gene_name, "\"", sep="")
  clusmat <- dbGetQuery(conn, sqlcom)
  clusnum <- as.integer(clusmat)
  
  # We can access the rows in the ratios matrix by calling ratios[tf[i],]
  # Here we are reading in the transcription factors
  tfNames <- scan(file="justTFS.txt", what="character", sep="\n")
  transFactors <- tfList(tfNames)
  
  #  Gets the mean of the clusters; default is to run on 5 clusters
  clusters <- getClusters(clusnum)
  
  # List to store the results
  results <- list()
  
  for (i in 1:NROW(clusters)){
    # y is a vector of the mean values of the cluster i
    y <- unlist(clusters[i,])
    
    #str(transFactors)
    
    # Here we pick the best correlated TFs to y, so that we don't have to
    # run bestFit with every transciption factor
    x <- predictors(y,transFactors)
    
    # finding the best/most parsimonious model using cv
    model <- bestFit(t(x),y)
    
    # add model to list of models for clusters
    results[[i]] <- model
    #str(model)
    cat("SUCCESS\n")
    pause()
  }
  
  #str(results)
  invisible(model)  
  
}

    
# The parameter numClust is the number of clusters that we want to fit a model to,
# this returns 5 vectors representing the means/mediods of each time point at each
# cluster.
getClusters <- function(clusnum=1){
	# Randomly selects cluster row numbers
	row.num <- clusnum
    
	# A list of vectors that contain the means of the cluster at each time point
	clusters <- list()
	for (i in row.num) {
    sqlcmd <- paste("select * from ba_ratios inner join k173 on ba_ratios.row_names=k173.row_names where out=", i, sep="")
 	  data <- dbGetQuery(conn, sqlcmd)
    
    # This will remove the non-numeric rows
    data<-data[-c(1,53,54)]
		
    # Taking the mean of the cluster and putting it into one vector
    cluster.mean <- as.vector(colMeans(data))
  	clusters <- rbind(clusters,cluster.mean)
  }
  cat("cluster shown: ", row.num, "\n")
  #str(clusters)
  invisible(clusters)
  
}


# This reads in the tf names and returns a matrix of only the TF values of

tfList <- function(tfNames){
  load("baa.ratios.rda")
  miniList <- list()
  
  for (i in 1:length(tfNames)){
    # as.vector will remove the names so we can work just with the numbers
    miniList[[i]] <- as.vector(ratios[tfNames[i],])
  }
  
  invisible(do.call(rbind,miniList))
}

  
# This returns the 20 most correlated (or anti-correlated b/c TFs can be
# correlated or anti correlated.
predictors <- function(y, tfs) {
  cors <- double()
    
  # Forming the list of correlations for each transcription
	for (i in 1:dim(tfs)[1]) {
		t = tfs[i,]
    tcor = abs(cor(y, t, use="na.or.complete"))
		cors <- append(cors, tcor)
	}
	
  # Like in the slides, taking the top 20 most correlated TFs
  top20 <- tfs[sort(cors, decreasing = T, index.return = T)$ix[1:20],]
	invisible(top20)
}
  
  
# Finds the most parsimonious model of x and y
bestFit <- function( x, y, kFolds=5, stepSize = .05, printSteps = FALSE){
  
  # Plotting the CV curve
  if (printSteps == FALSE){
    cv.lars(x, y, K=kFolds)
  }
  
  # Converting the step size to a range
  convertToRange = 1/stepSize
  bounds = (1:convertToRange)/convertToRange
  
  cv <- numeric(convertToRange)
  cv.err <- numeric(convertToRange)
  
  # Arbitrary Large Numbers to track best values so far
  mincv <- 10000
  mincv.err <- 10000
  bestT <- 0
  bestNumPredictors <- 0
  
  # Stepping through every shrinkage parameter
  for (i in 1:length(bounds)){ 
    l1ce.example  <- l1ce( y ~ x , sweep.out = ~ 1, standardize = TRUE,
                        bound = bounds[i], absolute.t = FALSE)
    pFromL1 <- which( abs(l1ce.example$coefficients) > 0)
    pFromL1 <- pFromL1[pFromL1 != 1] - 1
    
    # Making a model from the predictors found with the L1 Shrinkage
    cv.obj <- cv.lm( y, x, k = kFolds, p = pFromL1 )
    cv[i] <- cv.obj$cv
    cv.err[i] <- cv.obj$cv.err
    
    if (printSteps == TRUE){
      cat("Shrinkage Parameter is:", bounds[i], "\n")
      cat("CV is:", cv.obj$cv, "\n")
      cat("CV.err is:", cv.obj$cv.err, "\n")
    }
    
    # Testing for best shrinkage parameter
    if(cv.obj$cv < mincv){
      mincv = cv.obj$cv
      mincv.err = cv.obj$cv.err
      bestT = bounds[i]
      bestNumPredictors = length(pFromL1)
      bestPredictors = pFromL1
    }
  }

  if (printSteps == TRUE){
    plot.cv.lm( 1:convertToRange , cv, cv.err )
    cat("Best CV:",mincv, "\n")
    cat("Best t:", bestT, "\n")
    cat("The Best number of Predictors was:", bestNumPredictors, "\n")
    cat("The predictors were:", "\n")
    print(bestPredictors)
  }
      
  # This is the first shrinkage parameter that's within the  best cv+cv.err
  foundT <- 0
      
  for (i in 1:length(bounds)){
    if(cv[i] < (mincv + mincv.err)){
      foundT <- i*.05
      break
    }
  }
  
  cat("The shrinkage parameter t is:", foundT, "\n")
   
  # This is returned
  l1ce.final  <- l1ce( y ~ x , sweep.out = ~ 1, standardize = TRUE,
                        bound = foundT, absolute.t = FALSE)
  predictors <- which( abs(l1ce.final$coefficients) > 0)
  predictors <- predictors[predictors != 1] - 1
      
  cat("The number of predictors is: ", length(predictors), "\n")

  if ( length(predictors) < dim(x)[2] ) { 
      x <- as.matrix( x[,predictors] )
  }
  
  # Refitting the Model with best parameters from l1ce()
  lm.final <- lm(y ~ x)
  plot( y, predict( lm.final ) )
  abline(0,1, col = 2, lwd = 3, lty = 2)
  #summary(lm.final)
  
  invisible(l1ce.final)
  
}


# **** HELPER FUNCTIONS ****

# Splits the data
cv.folds <- function(n, folds = 10) {
  split(sample(1:n), rep(1:folds, length = n))
}


# Fits a lm to the data using cv
cv.lm <- function( y, x, k= 5, p = 1:dim(x)[2], method = "rss" ) {   
   if ( length(p) < dim(x)[2] ) { 
      x <- as.matrix( x[,p] )
   }   
   cv.subsets <- cv.folds( length(y), folds = k)
   cv.rss <- numeric(k)
   for (i in 1:k) {
   	  tmp.lm <- lm( y[ - cv.subsets[[i]] ] ~ x[ - cv.subsets[[i]],  ] )
   	  y.hat <- predict.from.lm( tmp.lm, x[cv.subsets[[i]],] )                                 
   	  cv.rss[i] <- mean( (y[ cv.subsets[[i]] ] - y.hat )**2 )  
   }	  
   return( list( cv = mean( cv.rss ), cv.err = sqrt( var( cv.rss ) / k ) ) ) 
}	


# Plots the lm
plot.cv.lm <- function (x, cv, cv.err ) {
    plot(x, cv, type = "b", ylim = range(cv, cv + cv.err, cv - cv.err))
    error.bars(x, cv + cv.err, cv - cv.err, width = 1/(1.5* length(x)) )
    invisible() 
}

 
# Predicts from the lm
predict.from.lm <- function( lm1, x) {
     x <- as.matrix( x )
     if (class(lm1) != "lm") {
       stop("input class is not lm")
     	return( FALSE )
     } 
     coeff <- lm1$coefficients
     n <- dim( x )[1]
     tmp.mat <- t( cbind( rep(1,n) , x) ) * coeff
     y.hat <-  apply( tmp.mat, 2, sum)
     invisible( y.hat ) 
}

# A simple  function
pause <- function() {  
  cat("Press <Enter> to continue...")
  readline()
  invisible()
}
